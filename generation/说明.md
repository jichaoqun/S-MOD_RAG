# 生成器部分
当前生成器部分使用大模型是毋庸置疑的，除非是你在很特殊的任务下使用知识图谱自己对搜索到的内容做重新的定义，否则大模型是当前最好的选择。
这里主要想说的是大型的选择与prompt工程
## 大模型选择
>我认为大模型选择就分为两中，有钱跟没钱
>**有钱**：直接上最新的、各家最好的模型，无论是本地部署还是使用API都可以，他的文本输入长度的支持、模型归纳总结的能力，都很强，因为我们最后的输出是大模型生成，他不能理解知识，给的知识在相关也白搭。
>**穷**：选一个能用的模型，值得是你的硬件资源可以跑起来的最大模型，能用就行，在本项目中测试的大模型使用的就是qwen2.5:0.5b的模型，效果还是有的，没办法太穷了。

## prompt工程
>上面的大模型很重要，但是让大模型能输出自己想要的东西更重要
>提示词工程给大模型的输入与输出都做了相关的约束，让模型更能输出你想要的内容
>**输入**：指定大模型的功能定义，以及相关知识的输入，让LLM能根据相关知识回答问题
>**输出**：指定大模型输出的格式，让大模型输出符合你的格式要求，比如，想要输出一个json格式的答案，那么你就要在prompt中指定输出格式，让大模型输出json格式的内容，并且只输出json格式的内容，其他之外的信息不要输出。